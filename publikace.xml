<?xml version="1.0" encoding="utf8"?>
<dodavka xmlns="urn:CZ-RVV-IS-VaV-XML-NS:data-1.2.9" struktura="RIV21A">
  <zahlavi>
    <rozsah>
      <informacni-oblast>RIV</informacni-oblast>
      <obdobi-sberu>2021</obdobi-sberu>
      <predkladatel>
        <subjekt>
          <druh>verejna-vysoka-skola</druh>
          <ICO>00216208</ICO>
          <nazev jazyk="#ORIG">Univerzita Karlova</nazev>
          <nazev jazyk="eng">Charles University</nazev>
          <nadrizena-organizacni-slozka-statu>MSM</nadrizena-organizacni-slozka-statu>
        </subjekt>
        <organizacni-jednotka>
          <kod>11320</kod>
          <nazev jazyk="#ORIG">Matematicko-fyzikální fakulta</nazev>
          <nazev jazyk="eng">Faculty of Mathematics and Physics</nazev>
        </organizacni-jednotka>
      </predkladatel>
    </rozsah>
    <dodavatel>
      <subjekt>
        <kod>MSM</kod>
      </subjekt>
      <pracovnik-povereny-pripravou-dodavky>
        <osoba>
          <cele-jmeno>Pavel Straňák</cele-jmeno>
          <kontakt>
            <telefonni-cislo druh="telefon">221 914 247</telefonni-cislo>
            <emailova-adresa>stranak@ufal.mff.cuni.cz</emailova-adresa>
          </kontakt>
        </osoba>
      </pracovnik-povereny-pripravou-dodavky>
    </dodavatel>
    <verze>01</verze>
    <pruvodka cislo-jednaci="1"/>
  </zahlavi>
  <obsah>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/UYR3ASNC" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>jpn</jazyk>
      <nazev>Universal Dependenciesにもとづく多言語係り受け可視化ツールdeplacy</nazev>
      <anotace>Universal Dependenciesは, カレル大学のLINDAT/CLARINを中心に製作中の多言語係り受けコーパスであり, 現在, 90の言語に及んでいる. Universal Dependenciesを用いた係り受け解析ツールや, 係り受け可視化ツールも数多く製作されており, グラフィカルな画面に解析結果が出力されるようになってきている. ただ, 係り受け解析作業そのものは, CUI上でpythonなどのスクリプト言語を用いておこなうのが主であり, できれば解析結果もCUI上で見たい. そのような要求に答えるべく, 可視化ツールdeplacyを製作した. deplacyは, Universal Dependenciesにもとづく係り受け有向グラフを, CUI上に表示するpython3モジュールである. さらに, 50以上の書写言語にdeplacyを適用し, 各言語用のデモページをGoogle Colaboratory上に製作した. https://koichiyasuoka.github.io/deplacy/で公開中である.</anotace>
      <odkaz>https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/259820</odkaz>
      <autori pocet-domacich="0" pocet-celkem="1">
        <autor je-domaci="false">
          <jmeno>孝一</jmeno>
          <prijmeni>安岡</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/VAY82WHA" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>en</jazyk>
      <nazev>Morphosyntactic predictability of translationese</nazev>
      <anotace>It is often assumed that translated texts are easier to process than original ones. However, it has also been shown that translated texts contain evident traces of source-language morphosyntax, which should presumably make them less predictable and harder to process. We test these competing observations by measuring morphosyntactic entropies of original and translated texts in several languages and show that there may exist a categorical distinction between translations made from structurally-similar languages (which are more predictable than original texts) and those made from structurally-divergent languages (which are often non-idiomatic, involve structural transfer, and therefore are more entropic).</anotace>
      <odkaz>https://www.degruyter.com/document/doi/10.1515/lingvan-2019-0077/html</odkaz>
      <doi>10.1515/lingvan-2019-0077</doi>
      <autori pocet-domacich="0" pocet-celkem="6">
        <autor je-domaci="false">
          <jmeno>Dmitry</jmeno>
          <prijmeni>Nikolaev</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Taelin</jmeno>
          <prijmeni>Karidi</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Neta</jmeno>
          <prijmeni>Kenneth</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Veronika</jmeno>
          <prijmeni>Mitnik</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Lilja</jmeno>
          <prijmeni>Saeboe</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Omri</jmeno>
          <prijmeni>Abend</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/CQRLDURF" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>DerivBase.Ru: a Derivational Morphology Resource for Russian</nazev>
      <anotace>Russian morphology has been studied for decades, but there is still no large high coverage resource that contains the derivational families (groups of words that share the same root) of Russian words. The number of words used in different areas of the language grows rapidly, thus the human-made dictionaries published long time ago cannot cover the neologisms and the domain-specific lexicons. To fill such resource gap, we have developed a rule-based framework for deriving words and we applied it to build a derivational morphology resource named DerivBase.Ru, which we introduce in this paper.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.485</odkaz>
      <autori pocet-domacich="0" pocet-celkem="1">
        <autor je-domaci="false">
          <jmeno>Daniil</jmeno>
          <prijmeni>Vodolazsky</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/D5XFARSD" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>Stress Test Evaluation of Transformer-based Models in Natural Language Understanding Tasks</nazev>
      <anotace>There has been significant progress in recent years in the field of Natural Language Processing thanks to the introduction of the Transformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have shown impressive results on several downstream tasks. Many researchers have studied previous (non-Transformer) models to understand their actual behavior under different scenarios, showing that these models are taking advantage of clues or failures of datasets and that slight perturbations on the input data can severely reduce their performance. In contrast, recent models have not been systematically tested with adversarial-examples in order to show their robustness under severe stress conditions. For that reason, this work evaluates three Transformer-based models (RoBERTa, XLNet, and BERT) in Natural Language Inference (NLI) and Question Answering (QA) tasks to know if they are more robust or if they have the same flaws as their predecessors. As a result, our experiments reveal that RoBERTa, XLNet and BERT are more robust than recurrent neural network models to stress tests for both NLI and QA tasks. Nevertheless, they are still very fragile and demonstrate various unexpected behaviors, thus revealing that there is still room for future improvement in this field.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.232</odkaz>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Carlos</jmeno>
          <prijmeni>Aspillaga</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Andrés</jmeno>
          <prijmeni>Carvallo</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Vladimir</jmeno>
          <prijmeni>Araujo</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/2C5KWRGT" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization</nazev>
      <anotace>The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages. In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task. Contrary to the findings of the 2018 shared task, we show, in experiments on the English 2018 dataset, that the use of synthetic data can have a substantial positive effect – an improvement of almost 8 BLEU points for a previously state-of-the-art system. We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.acl-main.665</odkaz>
      <doi>10.18653/v1/2020.acl-main.665</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Henry</jmeno>
          <prijmeni>Elder</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Robert</jmeno>
          <prijmeni>Burke</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Alexander</jmeno>
          <prijmeni>O'Connor</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Jennifer</jmeno>
          <prijmeni>Foster</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/2UBMF2RX" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank</nazev>
      <anotace>Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models' pretraining data and target language varieties.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.findings-emnlp.118</odkaz>
      <doi>10.18653/v1/2020.findings-emnlp.118</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Ethan C.</jmeno>
          <prijmeni>Chau</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Lucy H.</jmeno>
          <prijmeni>Lin</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Noah A.</jmeno>
          <prijmeni>Smith</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/4HNZZHPS" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>LSCP: Enhanced Large Scale Colloquial Persian Language Understanding</nazev>
      <anotace>Language recognition has been significantly advanced in recent years by means of modern machine learning methods such as deep learning and benchmarks with rich annotations. However, research is still limited in low-resource formal languages. This consists of a significant gap in describing the colloquial language especially for low-resourced ones such as Persian. In order to target this gap for low resource languages, we propose a “Large Scale Colloquial Persian Dataset” (LSCP). LSCP is hierarchically organized in a semantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. This encompasses the recognition of multiple semantic aspects in the human-level sentences, which naturally captures from the real-world sentences. We believe that further investigations and processing, as well as the application of novel algorithms and methods, can strengthen enriching computerized understanding and processing of low resource languages. The proposed corpus consists of 120M sentences resulted from 27M tweets annotated with parsing tree, part-of-speech tags, sentiment polarity and translation in five different languages.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.776</odkaz>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Hadi</jmeno>
          <prijmeni>Abdi Khojasteh</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ebrahim</jmeno>
          <prijmeni>Ansari</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Mahdi</jmeno>
          <prijmeni>Bohlouli</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/TQAIGEAY" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Load What You Need: Smaller Versions of Mutililingual BERT</nazev>
      <anotace>Pre-trained Transformer-based models are achieving state-of-the-art results on a variety of Natural Language Processing data sets. However, the size of these models is often a drawback for their deployment in real production applications. In the case of multilingual models, most of the parameters are located in the embeddings layer. Therefore, reducing the vocabulary size should have an important impact on the total number of parameters. In this paper, we propose to extract smaller models that handle fewer number of languages according to the targeted corpora. We present an evaluation of smaller versions of multilingual BERT on the XNLI data set, but we believe that this method may be applied to other multilingual transformers. The obtained results confirm that we can generate smaller models that keep comparable results, while reducing up to 45% of the total number of parameters. We compared our models with DistilmBERT (a distilled version of multilingual BERT) and showed that unlike language reduction, distillation induced a 1.7% to 6% drop in the overall accuracy on the XNLI data set. The presented models and code are publicly available.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.sustainlp-1.16</odkaz>
      <doi>10.18653/v1/2020.sustainlp-1.16</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Amine</jmeno>
          <prijmeni>Abdaoui</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Camille</jmeno>
          <prijmeni>Pradel</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Grégoire</jmeno>
          <prijmeni>Sigel</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/GLZG5BI3" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Liputan6: A Large-scale Indonesian Dataset for Text Summarization</nazev>
      <anotace>In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from Liputan6.com, an online news portal, and obtain 215,827 document–summary pairs. We leverage pre-trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT-based models. We include a thorough error analysis by examining machine-generated summaries that have low ROUGE scores, and expose both issues with ROUGE itself, as well as with extractive and abstractive summarization models.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.aacl-main.60</odkaz>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Fajri</jmeno>
          <prijmeni>Koto</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Jey Han</jmeno>
          <prijmeni>Lau</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Timothy</jmeno>
          <prijmeni>Baldwin</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/ZBLJW9SH" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Køpsala: Transition-Based Graph Parsing via Efficient Training and Effective Encoding</nazev>
      <anotace>We present Køpsala, the Copenhagen-Uppsala system for the Enhanced Universal Dependencies Shared Task at IWPT 2020. Our system is a pipeline consisting of off-the-shelf models for everything but enhanced graph parsing, and for the latter, a transition-based graph parser adapted from Che et al. (2019). We train a single enhanced parser model per language, using gold sentence splitting and tokenization for training, and rely only on tokenized surface forms and multilingual BERT for encoding. While a bug introduced just before submission resulted in a severe drop in precision, its post-submission fix would bring us to 4th place in the official ranking, according to average ELAS. Our parser demonstrates that a unified pipeline is effective for both Meaning Representation Parsing and Enhanced Universal Dependencies.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.iwpt-1.25</odkaz>
      <doi>10.18653/v1/2020.iwpt-1.25</doi>
      <autori pocet-domacich="0" pocet-celkem="5">
        <autor je-domaci="false">
          <jmeno>Daniel</jmeno>
          <prijmeni>Hershcovich</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Miryam</jmeno>
          <prijmeni>Lhoneux</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Artur</jmeno>
          <prijmeni>Kulmizev</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Elham</jmeno>
          <prijmeni>Pejhan</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Joakim</jmeno>
          <prijmeni>Nivre</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/F28EQAP3" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Intelligenti Pauca - Probing a Novel Alternative to Universal Dependencies for Under-Resourced Languages on Latin</nazev>
      <anotace>In this paper, we aim at improving the study of Latin in three ways: 1) by providing better visualizations of syntagma and structure for both research and the classroom, 2) by supporting a
high-level search interface for corpus exploration, and 3) by improving the accuracy of taggers
and parsers. To achieve this, we introduce a new linguistic description called Intelligenti Pauca,
an alternative to Universal Dependencies for under-resourced languages. We show the key differences between the two linguistic descriptions, how the structure of Intelligenti Pauca favours
our goals, and the effect it has on parsing accuracy for the Index Tomisticus Treebank.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.tlt-1.10</odkaz>
      <doi>10.18653/v1/2020.tlt-1.10</doi>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Daniel Couto</jmeno>
          <prijmeni>Vale</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Konstantin</jmeno>
          <prijmeni>Schulz</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/3BD4XMHD" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding</nazev>
      <anotace>Although Indonesian is known to be the fourth most frequently used language over the internet, the research progress on this language in natural language processing (NLP) is slow-moving due to a lack of available resources. In response, we introduce the first-ever vast resource for training, evaluation, and benchmarking on Indonesian natural language understanding (IndoNLU) tasks. IndoNLU includes twelve tasks, ranging from single sentence classification to pair-sentences sequence labeling with different levels of complexity. The datasets for the tasks lie in different domains and styles to ensure task diversity. We also provide a set of Indonesian pre-trained models (IndoBERT) trained from a large and clean Indonesian dataset (Indo4B) collected from publicly available sources such as social media texts, blogs, news, and websites. We release baseline models for all twelve tasks, as well as the framework for benchmark evaluation, thus enabling everyone to benchmark their system performances.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.aacl-main.85</odkaz>
      <autori pocet-domacich="0" pocet-celkem="11">
        <autor je-domaci="false">
          <jmeno>Bryan</jmeno>
          <prijmeni>Wilie</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Karissa</jmeno>
          <prijmeni>Vincentio</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Genta Indra</jmeno>
          <prijmeni>Winata</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Samuel</jmeno>
          <prijmeni>Cahyawijaya</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Xiaohong</jmeno>
          <prijmeni>Li</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Zhi Yuan</jmeno>
          <prijmeni>Lim</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Sidik</jmeno>
          <prijmeni>Soleman</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Rahmad</jmeno>
          <prijmeni>Mahendra</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Pascale</jmeno>
          <prijmeni>Fung</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Syafri</jmeno>
          <prijmeni>Bahar</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ayu</jmeno>
          <prijmeni>Purwarianti</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/JPWN89TF" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP</nazev>
      <anotace>Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.coling-main.66</odkaz>
      <doi>10.18653/v1/2020.coling-main.66</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Fajri</jmeno>
          <prijmeni>Koto</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Afshin</jmeno>
          <prijmeni>Rahimi</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Jey Han</jmeno>
          <prijmeni>Lau</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Timothy</jmeno>
          <prijmeni>Baldwin</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/LY8TXSGI" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>spa</jazyk>
      <nazev>Explainable OpenIE Classifier with Morpho-syntactic Rules</nazev>
      <anotace>Autorías: Bruno Cabral, Marlo Souza, Daniela Barreiro Claro.
Localización: Proceedings of the Workshop on Hybrid Intelligence for Natural Language Processing Tasks (HI4NLP 2020) co-located with 24th European Conference on Artificial Intelligence (ECAI 2020): Santiago de Compostela, Spain, August 29, 2020, 2020.
Artículo de Libro en Dialnet.</anotace>
      <odkaz>https://dialnet.unirioja.es/servlet/articulo?codigo=7670688</odkaz>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Bruno</jmeno>
          <prijmeni>Cabral</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Marlo</jmeno>
          <prijmeni>Souza</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Daniela Barreiro</jmeno>
          <prijmeni>Claro</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/MDMGSSM2" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Do Neural Language Models Show Preferences for Syntactic Formalisms?</nazev>
      <anotace>Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.acl-main.375</odkaz>
      <doi>10.18653/v1/2020.acl-main.375</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Artur</jmeno>
          <prijmeni>Kulmizev</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Vinit</jmeno>
          <prijmeni>Ravishankar</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Mostafa</jmeno>
          <prijmeni>Abdou</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Joakim</jmeno>
          <prijmeni>Nivre</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/YU6G6SED" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus</nazev>
      <anotace>Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.acl-main.627</odkaz>
      <doi>10.18653/v1/2020.acl-main.627</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Hao</jmeno>
          <prijmeni>Fei</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Meishan</jmeno>
          <prijmeni>Zhang</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Donghong</jmeno>
          <prijmeni>Ji</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/B4YLQR6B" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Addressing Exposure Bias With Document Minimum Risk Training: Cambridge at the WMT20 Biomedical Translation Task</nazev>
      <anotace>The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove `problem' training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.wmt-1.94</odkaz>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Danielle</jmeno>
          <prijmeni>Saunders</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Bill</jmeno>
          <prijmeni>Byrne</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/U6XGUKFM" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>en</jazyk>
      <nazev>A Neural Approach to Discourse Relation Signal Detection</nazev>
      <anotace>Previous data-driven work investigating the types and distributions of discourse relation signals, including discourse markers such as 'however' or phrases such as 'as a result' has focused on the relative frequencies of signal words within and outside text from each discourse relation. Such approaches do not allow us to quantify the signaling strength of individual instances of a signal on a scale (e.g. more or less discourse-relevant instances of 'and'), to assess the distribution of ambiguity for signals, or to identify words that hinder discourse relation identification in context ('anti-signals' or 'distractors'). In this paper we present a data-driven approach to signal detection using a distantly supervised neural network and develop a metric, Δs (or 'delta-softmax'), to quantify signaling strength. Ranging between -1 and 1 and relying on recent advances in contextualized words embeddings, the metric represents each word's positive or negative contribution to the identifiability of a relation in specific instances in context. Based on an English corpus annotated for discourse relations using Rhetorical Structure Theory and signal type annotations anchored to specific tokens, our analysis examines the reliability of the metric, the places where it overlaps with and differs from human judgments, and the implications for identifying features that neural models may need in order to perform better on automatic discourse relation classification.</anotace>
      <odkaz>https://journals.uic.edu/ojs/index.php/dad/article/view/11372</odkaz>
      <doi>10.5087/dad.2020.201</doi>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Amir</jmeno>
          <prijmeni>Zeldes</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Yang</jmeno>
          <prijmeni>Liu</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/CPZHU5YH" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>en</jazyk>
      <nazev>Estimating word-level quality of statistical machine translation output using monolingual information alone</nazev>
      <anotace>Various studies show that statistical machine translation (SMT) systems suffer from fluency errors, especially in the form of grammatical errors and errors related to idiomatic word choices. In this study, we investigate the effectiveness of using monolingual information contained in the machine-translated text to estimate word-level quality of SMT output. We propose a recurrent neural network architecture which uses morpho-syntactic features and word embeddings as word representations within surface and syntactic n-grams. We test the proposed method on two language pairs and for two tasks, namely detecting fluency errors and predicting overall post-editing effort. Our results show that this method is effective for capturing all types of fluency errors at once. Moreover, on the task of predicting post-editing effort, while solely relying on monolingual information, it achieves on-par results with the state-of-the-art quality estimation systems which use both bilingual and monolingual information.</anotace>
      <odkaz>https://www.cambridge.org/core/journals/natural-language-engineering/article/estimating-wordlevel-quality-of-statistical-machine-translation-output-using-monolingual-information-alone/CC59FF0C07E859AAA01CC30CF7BA9326</odkaz>
      <doi>10.1017/S1351324919000111</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Arda</jmeno>
          <prijmeni>Tezcan</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Véronique</jmeno>
          <prijmeni>Hoste</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Lieve</jmeno>
          <prijmeni>Macken</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/SBISS5NF" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Word class flexibility: A deep contextualized approach</nazev>
      <anotace>Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages. We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.emnlp-main.71</odkaz>
      <doi>10.18653/v1/2020.emnlp-main.71</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Bai</jmeno>
          <prijmeni>Li</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Guillaume</jmeno>
          <prijmeni>Thomas</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Yang</jmeno>
          <prijmeni>Xu</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Frank</jmeno>
          <prijmeni>Rudzicz</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/3TW784BQ" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>Wikinflection Corpus: A (Better) Multilingual, Morpheme-Annotated Inflectional Corpus</nazev>
      <anotace>Multilingual, inflectional corpora are a scarce resource in the NLP community, especially corpora with annotated morpheme boundaries. We are evaluating a generated, multilingual inflectional corpus with morpheme boundaries, generated from the English Wiktionary (Metheniti and Neumann, 2018), against the largest, multilingual, high-quality inflectional corpus of the UniMorph project (Kirov et al., 2018). We confirm that the generated Wikinflection corpus is not of such quality as UniMorph, but we were able to extract a significant amount of words from the intersection of the two corpora. Our Wikinflection corpus benefits from the morpheme segmentations of Wiktionary/Wikinflection and from the manually-evaluated morphological feature tags of the UniMorph project, and has 216K lemmas and 5.4M word forms, in a total of 68 languages.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.481</odkaz>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Eleni</jmeno>
          <prijmeni>Metheniti</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Guenter</jmeno>
          <prijmeni>Neumann</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/SJYPX28B" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>Towards the Conversion of National Corpus of Polish to Universal Dependencies</nazev>
      <anotace>The research presented in this paper aims at enriching the manually morphosyntactically annotated part of National Corpus of Polish (NKJP1M) with a syntactic layer, i.e. dependency trees of sentences, and at converting both dependency trees and morphosyntactic annotations of particular tokens to Universal Dependencies. The dependency layer is built using a semi-automatic annotation procedure. The sentences from NKJP1M are first parsed with a dependency parser trained on Polish Dependency Bank, i.e. the largest bank of Polish dependency trees. The predicted dependency trees and the morphosyntactic annotations of tokens are then automatically converted into UD dependency graphs. NKJP1M sentences are an essential part of Polish Dependency Bank, we thus replace some automatically predicted dependency trees with their manually annotated equivalents. The final dependency treebank consists of 86K trees (including 15K gold-standard trees). A natural language pre-processing model trained on the enlarged set of (possibly noisy) dependency trees outperforms a model trained on a smaller set of the gold-standard trees in predicting part-of-speech tags, morphological features, lemmata, and labelled dependency trees</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.653</odkaz>
      <autori pocet-domacich="0" pocet-celkem="1">
        <autor je-domaci="false">
          <jmeno>Alina</jmeno>
          <prijmeni>Wróblewska</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/LCN4RFF8" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>en</jazyk>
      <nazev>Recognizing Preferred Grammatical Gender in Russian Anonymous Online Confessions</nazev>
      <anotace>We present annotation results for a dataset of public anonymous online confessions in Russian (“Overheard/Podslushano” group in VKontakte, posts tagged #family). Unlike many other cases with online social network data, intentionally anonymous posts do not contain any explicit metadata such as age or gender. We consider the problem of predicting the author’s preferred grammatical gender for self-reference, a problem that proved to be surprisingly hard and not reducible to simple morphological analysis. We describe an expert labeling of a dataset for this problem, show the findings of predictive analysis, and introduce rule-based and machine learning approaches.</anotace>
      <doi>10.1007/978-3-030-58323-1_24</doi>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Anton</jmeno>
          <prijmeni>Alekseev</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Sergey</jmeno>
          <prijmeni>Nikolenko</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/3AZ2SYIN" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>UniMorph 3.0: Universal Morphology</nazev>
      <anotace>The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.483</odkaz>
      <autori pocet-domacich="0" pocet-celkem="22">
        <autor je-domaci="false">
          <jmeno>Arya D.</jmeno>
          <prijmeni>McCarthy</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Christo</jmeno>
          <prijmeni>Kirov</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Matteo</jmeno>
          <prijmeni>Grella</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Amrit</jmeno>
          <prijmeni>Nidhi</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Patrick</jmeno>
          <prijmeni>Xia</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Kyle</jmeno>
          <prijmeni>Gorman</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ekaterina</jmeno>
          <prijmeni>Vylomova</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Sabrina J.</jmeno>
          <prijmeni>Mielke</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Garrett</jmeno>
          <prijmeni>Nicolai</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Miikka</jmeno>
          <prijmeni>Silfverberg</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Timofey</jmeno>
          <prijmeni>Arkhangelskiy</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Nataly</jmeno>
          <prijmeni>Krizhanovsky</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Andrew</jmeno>
          <prijmeni>Krizhanovsky</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Elena</jmeno>
          <prijmeni>Klyachko</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Alexey</jmeno>
          <prijmeni>Sorokin</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>John</jmeno>
          <prijmeni>Mansfield</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Valts</jmeno>
          <prijmeni>Ernštreits</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Yuval</jmeno>
          <prijmeni>Pinter</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Cassandra L.</jmeno>
          <prijmeni>Jacobs</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ryan</jmeno>
          <prijmeni>Cotterell</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Mans</jmeno>
          <prijmeni>Hulden</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>David</jmeno>
          <prijmeni>Yarowsky</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/VGF25ATR" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Syntactic Complexity of Learning Content in Italian for COVID-19 Frontline Responders: A Study on WHO’s Emergency Learning Platform</nazev>
      <odkaz>https://www.zurnalai.vu.lt/verbum/article/view/21210</odkaz>
      <doi>10.15388/Verb.15</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Giuseppe</jmeno>
          <prijmeni>Samo</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ursula Yu</jmeno>
          <prijmeni>Zhao</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Gaya</jmeno>
          <prijmeni>Gamhewage</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/F3EEZV4C" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Subjects tend to be coded only once: Corpus-based and grammar-based evidence for an efficiency-driven trade-off</nazev>
      <odkaz>https://www.aclweb.org/anthology/2020.tlt-1.8</odkaz>
      <doi>10.18653/v1/2020.tlt-1.8</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>Aleksandrs</jmeno>
          <prijmeni>Berdicevskis</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Karsten</jmeno>
          <prijmeni>Schmidtke-Bode</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ilja</jmeno>
          <prijmeni>Seržant</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/9FN572R8" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>en</jazyk>
      <nazev>ParaDis and Démonette – From Theory to Resources for Derivational Paradigms</nazev>
      <anotace>In this article, we trace the genesis of the French derivational database Démonette and show how its architecture and content stem from recent theoretical developments in derivational morphology and from user needs. The development of this large-scale resource began a year ago as part of the Demonext project. Its conception is grounded in a theoretical approach where the lexemes are connected by derivational relations within derivational families which in turn fit into paradigms. More precisely, Démonette is a partial implementation of ParaDis, a paradigmatic model of morphological representation designed for the description of regular processes and of form-meaning discrepancies. The article focuses on the principles that govern the morphological, structural and semantic encoding of morphologically complex lexemes in Démonette and illustrates the range of form-meaning discrepancies with a variety of examples of non-canonical word formations.</anotace>
      <odkaz>https://halshs.archives-ouvertes.fr/halshs-02707601</odkaz>
      <doi>10.14712/00326585.001</doi>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Fiammetta</jmeno>
          <prijmeni>Namer</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Nabil</jmeno>
          <prijmeni>Hathout</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/XWCX7NR5" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions</nazev>
      <anotace>A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages' gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.emnlp-main.456</odkaz>
      <doi>10.18653/v1/2020.emnlp-main.456</doi>
      <autori pocet-domacich="0" pocet-celkem="5">
        <autor je-domaci="false">
          <jmeno>Arya D.</jmeno>
          <prijmeni>McCarthy</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Adina</jmeno>
          <prijmeni>Williams</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Shijia</jmeno>
          <prijmeni>Liu</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>David</jmeno>
          <prijmeni>Yarowsky</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ryan</jmeno>
          <prijmeni>Cotterell</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/UINQ7QNH" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>LINSPECTOR: Multilingual Probing Tasks for Word Representations</nazev>
      <anotace>Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.</anotace>
      <odkaz>https://doi.org/10.1162/coli_a_00376</odkaz>
      <doi>10.1162/coli_a_00376</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Gözde Gül</jmeno>
          <prijmeni>Şahin</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Clara</jmeno>
          <prijmeni>Vania</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ilia</jmeno>
          <prijmeni>Kuznetsov</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Iryna</jmeno>
          <prijmeni>Gurevych</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/EDF7GQ2X" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models</nazev>
      <anotace>The National Research Council of Canada's team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting sentence pairs from document pairs and (2) a crosslingual semantic textual similarity metric based on a pretrained multilingual language model, XLM-RoBERTa, with bilingual mappings learnt from a minimal amount of clean parallel data for scoring the parallelism of the extracted sentence pairs. The translation quality of the neural machine translation systems trained and fine-tuned on the parallel data extracted by our submissions improved significantly when compared to the organizers' LASER-based baseline, a sentence-embedding method that worked well last year. For re-aligning the sentences in the document pairs (component 1), our statistical approach has outperformed the current state-of-the-art neural approach in this low-resource context.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.wmt-1.110</odkaz>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Chi-kiu</jmeno>
          <prijmeni>Lo</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Eric</jmeno>
          <prijmeni>Joanis</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/KQR6ARID" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>How Universal are Universal Dependencies? Exploiting Syntax for Multilingual Clause-level Sentiment Detection</nazev>
      <anotace>This paper investigates clause-level sentiment detection in a multilingual scenario. Aiming at a high-precision, fine-grained, configurable, and non-biased system for practical use cases, we have designed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, avoiding machine-learning approaches that may cause obstacles to our purposes. We achieved high precision in sentiment detection for 17 languages and identified the advantages of common syntactic structures as well as issues stemming from structural differences on Universal Dependencies. In addition to reusable tips for handling multilingual syntax, we provide a parallel benchmarking data set for further research.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.500</odkaz>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Hiroshi</jmeno>
          <prijmeni>Kanayama</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ran</jmeno>
          <prijmeni>Iwamoto</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/34J58YHQ" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment</nazev>
      <odkaz>https://hal.archives-ouvertes.fr/hal-03018261</odkaz>
      <doi>10.18653/v1/P17</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Hartger</jmeno>
          <prijmeni>Veeman</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Marc</jmeno>
          <prijmeni>Allassonnière-Tang</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Aleksandrs</jmeno>
          <prijmeni>Berdicevskis</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ali</jmeno>
          <prijmeni>Basirat</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/MTG3QMXT" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>GRAIN-S: Manually Annotated Syntax for German Interviews</nazev>
      <anotace>We present GRAIN-S, a set of manually created syntactic annotations for radio interviews in German. The dataset extends an existing corpus GRAIN and comes with constituency and dependency trees for six interviews. The rare combination of gold- and silver-standard annotation layers coming from GRAIN with high-quality syntax trees can serve as a useful resource for speech- and text-based research. Moreover, since interviews can be put between carefully prepared speech and spontaneous conversational speech, they cover phenomena not seen in traditional newspaper-based treebanks. Therefore, GRAIN-S can contribute to research into techniques for model adaptation and for building more corpus-independent tools. GRAIN-S follows TIGER, one of the established syntactic treebanks of German. We describe the annotation process and discuss decisions necessary to adapt the original TIGER guidelines to the interviews domain. Next, we give details on the conversion from TIGER-style trees to dependency trees. We provide data statistics and demonstrate differences between the new dataset and existing out-of-domain test sets annotated with TIGER syntactic structures. Finally, we provide baseline parsing results for further comparison.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.636</odkaz>
      <autori pocet-domacich="0" pocet-celkem="6">
        <autor je-domaci="false">
          <jmeno>Agnieszka</jmeno>
          <prijmeni>Falenska</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Zoltán</jmeno>
          <prijmeni>Czesznak</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Kerstin</jmeno>
          <prijmeni>Jung</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Moritz</jmeno>
          <prijmeni>Völkel</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Wolfgang</jmeno>
          <prijmeni>Seeker</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Jonas</jmeno>
          <prijmeni>Kuhn</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/GJ7IX6IN" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>en</jazyk>
      <nazev>Deep Neural Networks Ensemble with Word Vector Representation Models to Resolve Coreference Resolution in Russian</nazev>
      <anotace>In this paper we present a novel neural networks ensemble to solve the task of coreference resolution in Russian texts. The ensemble consists of several neural networks, each based on recurrent Bidirectional long short-term memory layers (BiLSTM), attention mechanism, consistent scoring with selection of probable mentions and antecedents. The applied neural network topology has already shown state-of-the-art results in English for this task, and is now adapted for the Russian language. The resulting coreference markup is obtained by aggregating output scores from several blocks of independently trained neural network models. To represent an input source text, a combination of word vectors from two language models is used. We study the dependence of the coreference detection accuracy on various combinations of models of vector representation of words along with two tokenization approaches: gold markup or UDpipe tools. Finally, to show the improvement made by our ensemble approach, we present the results of experiments with both RuCor and AnCor datasets.</anotace>
      <doi>10.1007/978-3-030-33491-8_4</doi>
      <autori pocet-domacich="0" pocet-celkem="3">
        <autor je-domaci="false">
          <jmeno>A.</jmeno>
          <prijmeni>Sboev</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>R.</jmeno>
          <prijmeni>Rybka</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>A.</jmeno>
          <prijmeni>Gryaznov</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/KP7YJ4ZE" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>Creating a Parallel Icelandic Dependency Treebank from Raw Text to Universal Dependencies</nazev>
      <anotace>Making the low-resource language, Icelandic, accessible and usable in Language Technology is a work in progress and is supported by the Icelandic government. Creating resources and suitable training data (e.g., a dependency treebank) is a fundamental part of that work. We describe work on a parallel Icelandic dependency treebank based on Universal Dependencies (UD). This is important because it is the first parallel treebank resource for the language and since several other languages already have a resource based on the same text. Two Icelandic treebanks based on phrase-structure grammar have been built and ongoing work aims to convert them to UD. Previously, limited work has been done on dependency grammar for Icelandic. The current project aims to ameliorate this situation by creating a small dependency treebank from scratch. Creating a treebank is a laborious task so the process was implemented in an accessible manner using freely available tools and resources. The parallel data in the UD project was chosen as a source because this would furthermore give us the first parallel treebank for Icelandic. The Icelandic parallel UD corpus will be published as part of UD version 2.6.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.357</odkaz>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Hildur</jmeno>
          <prijmeni>Jónsdóttir</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Anton Karl</jmeno>
          <prijmeni>Ingason</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/3ESVT7W6" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>BMEAUT at SemEval-2020 Task 2: Lexical Entailment with Semantic Graphs</nazev>
      <anotace>In this paper we present a novel rule-based, language independent method for determining lexical entailment relations using semantic representations built from Wiktionary definitions. Combined with a simple WordNet-based method our system achieves top scores on the English and Italian datasets of the Semeval-2020 task “Predicting Multilingual and Cross-lingual (graded) Lexical Entailment” (Glavaš et al., 2020). A detailed error analysis of our output uncovers future di- rections for improving both the semantic parsing method and the inference process on semantic graphs.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.semeval-1.15</odkaz>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Ádám</jmeno>
          <prijmeni>Kovács</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Kinga</jmeno>
          <prijmeni>Gémes</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Andras</jmeno>
          <prijmeni>Kornai</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Gábor</jmeno>
          <prijmeni>Recski</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/YN3FKAPB" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>A span-graph neural model for overlapping entity relation extraction in biomedical texts</nazev>
      <anotace>Entity relation extraction is one of the fundamental tasks in biomedical text mining, which is usually solved by the models from natural language processing. Compared with traditional pipeline methods, joint methods can avoid the error propagation from entity to relation, giving better performances. However, the existing joint models are built upon sequential scheme, and fail to detect overlapping entity and relation, which are ubiquitous in biomedical texts. The main reason is that sequential models have relatively weaker power in capturing long-range dependencies, which results in lower performance in encoding longer sentences. In this article, we propose a novel span-graph neural model for jointly extracting overlapping entity relation in biomedical texts. Our model treats the task as relation triplets prediction, and builds the entity-graph by enumerating possible candidate entity spans. The proposed model captures the relationship between the correlated entities via a span scorer and a relation scorer, respectively, and finally outputs all valid relational triplets.Experimental results on two biomedical entity relation extraction tasks, including drug–drug interaction detection and protein–protein interaction detection, show that the proposed method outperforms previous models by a substantial margin, demonstrating the effectiveness of span-graph-based method for overlapping relation extraction in biomedical texts. Further in-depth analysis proves that our model is more effective in capturing the long-range dependencies for relation extraction compared with the sequential models.Related codes are made publicly available at http://github.com/Baxelyne/SpanBioER.</anotace>
      <odkaz>https://doi.org/10.1093/bioinformatics/btaa993</odkaz>
      <doi>10.1093/bioinformatics/btaa993</doi>
      <autori pocet-domacich="0" pocet-celkem="4">
        <autor je-domaci="false">
          <jmeno>Hao</jmeno>
          <prijmeni>Fei</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Yue</jmeno>
          <prijmeni>Zhang</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Yafeng</jmeno>
          <prijmeni>Ren</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Donghong</jmeno>
          <prijmeni>Ji</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/NJLXMGFH" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>A Graph Based Framework for Structured Prediction Tasks in Sanskrit</nazev>
      <anotace>We propose a framework using Energy Based Models for multiple structured prediction tasks in Sanskrit. Ours is an arc-factored model, similar to the graph-based parsing approaches, and we consider the tasks of word segmentation, morphological parsing, dependency parsing, syntactic linearization, and prosodification, a prosody-level task we introduce in this work. Ours is a search-based structured prediction framework, which expects a graph as input, where relevant linguistic information is encoded in the nodes, and the edges are then used to indicate the association between these nodes. Typically, the state of the art models for morphosyntactic tasks in morphologically rich languages still rely on hand-crafted features for their performance. But here, we automate the learning of the feature function. The feature function so learned, along with the search space we construct, encode relevant linguistic information for the tasks we consider. This enables us to substantially reduce the training data requirements to as low as 10 % as compared to the data requirements for the neural state of the art models. Our experiments in Czech and Sanskrit show the language-agnostic nature of the framework, where we train highly competitive models for both the languages. Moreover, our framework enables us to incorporate languagespecific constraints to prune the search space and to filter the candidates during inference. We obtain significant improvements in morphosyntactic tasks for Sanskrit by incorporating language-specific constraints into the model. In all the tasks we discuss for Sanskrit, we either achieve state of the art results or ours is the only data-driven solution for those tasks.</anotace>
      <odkaz>https://doi.org/10.1162/coli_a_00390</odkaz>
      <doi>10.1162/coli_a_00390</doi>
      <autori pocet-domacich="0" pocet-celkem="5">
        <autor je-domaci="false">
          <jmeno>Amrith</jmeno>
          <prijmeni>Krishna</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Bishal</jmeno>
          <prijmeni>Santra</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Ashim</jmeno>
          <prijmeni>Gupta</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Pavankumar</jmeno>
          <prijmeni>Satuluri</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Pawan</jmeno>
          <prijmeni>Goyal</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/XKZW4TM3" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>A Gradient Boosting-Seq2Seq System for Latin POS Tagging and Lemmatization</nazev>
      <anotace>The paper presents the system used in the EvaLatin shared task to POS tag and lemmatize Latin. It consists of two components. A gradient boosting machine (LightGBM) is used for POS tagging, mainly fed with pre-computed word embeddings of a window of seven contiguous tokens—the token at hand plus the three preceding and following ones—per target feature value. Word embeddings are trained on the texts of the Perseus Digital Library, Patrologia Latina, and Biblioteca Digitale di Testi Tardo Antichi, which together comprise a high number of texts of different genres from the Classical Age to Late Antiquity. Word forms plus the outputted POS labels are used to feed a seq2seq algorithm implemented in Keras to predict lemmas. The final shared-task accuracies measured for Classical Latin texts are in line with state-of-the-art POS taggers (∼0.96) and lemmatizers (∼0.95).</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lt4hala-1.19</odkaz>
      <autori pocet-domacich="0" pocet-celkem="1">
        <autor je-domaci="false">
          <jmeno>Giuseppe G. A.</jmeno>
          <prijmeni>Celano</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/C5TBI33Q" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>English</jazyk>
      <nazev>A Diachronic Treebank of Russian Spanning More Than a Thousand Years</nazev>
      <anotace>We describe the Tromsø Old Russian and Old Church Slavonic Treebank (TOROT) that spans from the earliest Old Church Slavonic to modern Russian texts, covering more than a thousand years of continuous language history. We focus on the latest additions to the treebank, first of all, the modern subcorpus that was created by a high-quality conversion of the existing treebank of contemporary standard Russian (SynTagRus).</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.lrec-1.646</odkaz>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Aleksandrs</jmeno>
          <prijmeni>Berdicevskis</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Hanne</jmeno>
          <prijmeni>Eckhoff</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/RFMZDT8A" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <jazyk>Slovak</jazyk>
      <nazev>Ako a prečo pomenovať to, čo robíme? Problém slovenského prekladu termínu „digital humanities“</nazev>
      <odkaz>https://www.ceeol.com/search/article-detail?id=923991</odkaz>
      <autori pocet-domacich="0" pocet-celkem="1">
        <autor je-domaci="false">
          <jmeno>Andrej</jmeno>
          <prijmeni>Gogora</prijmeni>
        </autor>
      </autori>
    </vysledek>
    <vysledek identifikacni-kod="http://zotero.org/groups/2792663/items/7TPTTB95" duvernost-udaju="verejne-pristupne" rok-uplatneni="2021" druh="ostatni">
      <nazev>A Pointer Network Architecture for Joint Morphological Segmentation and Tagging</nazev>
      <anotace>Morphologically Rich Languages (MRLs) such as Arabic, Hebrew and Turkish often require Morphological Disambiguation (MD), i.e., the prediction of morphological decomposition of tokens into morphemes, early in the pipeline. Neural MD may be addressed as a simple pipeline, where segmentation is followed by sequence tagging, or as an end-to-end model, predicting morphemes from raw tokens. Both approaches are sub-optimal; the former is heavily prone to error propagation, and the latter does not enjoy explicit access to the basic processing units called morphemes. This paper offers MD architecture that combines the symbolic knowledge of morphemes with the learning capacity of neural end-to-end modeling. We propose a new, general and easy-to-implement Pointer Network model where the input is a morphological lattice and the output is a sequence of indices pointing at a single disambiguated path of morphemes. We demonstrate the efficacy of the model on segmentation and tagging, for Hebrew and Turkish texts, based on their respective Universal Dependencies (UD) treebanks. Our experiments show that with complete lattices, our model outperforms all shared-task results on segmenting and tagging these languages. On the SPMRL treebank, our model outperforms all previously reported results for Hebrew MD in realistic scenarios.</anotace>
      <odkaz>https://www.aclweb.org/anthology/2020.findings-emnlp.391</odkaz>
      <doi>10.18653/v1/2020.findings-emnlp.391</doi>
      <autori pocet-domacich="0" pocet-celkem="2">
        <autor je-domaci="false">
          <jmeno>Amit</jmeno>
          <prijmeni>Seker</prijmeni>
        </autor>
        <autor je-domaci="false">
          <jmeno>Reut</jmeno>
          <prijmeni>Tsarfaty</prijmeni>
        </autor>
      </autori>
    </vysledek>
  </obsah>
</dodavka>
